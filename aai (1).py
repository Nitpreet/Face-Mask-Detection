# -*- coding: utf-8 -*-
"""AAI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q5nJ5e_RY8RoUgyrDysrSZ2cn3V3y05Y
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
from torchvision.datasets import ImageFolder
from torchvision.transforms.transforms import Normalize
from sklearn.model_selection import train_test_split as sklearn_train_test_split
from sklearn.metrics import confusion_matrix
from operator import truediv
import numpy as np
import seaborn as sn
import pandas as pd
import warnings
warnings.filterwarnings("ignore")
import math
from sklearn.metrics import classification_report
from sklearn.model_selection import KFold
from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset

classes = ('Cloth_Mask', 'N95', 'N95WithValve', 'Surgical_Mask','Without_Mask')

Reshape_Image = transforms.Resize((64,64))
Image_to_tensor = torchvision.transforms.ToTensor()
Normalize_Image = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))

transform = transforms.Compose([Reshape_Image,Image_to_tensor,Normalize_Image])

def import_data():
  return ImageFolder(root = "/content/drive/MyDrive/FINAL_DATASET",transform = transform)

def my_train_test_split(dataset):
  train, test = sklearn_train_test_split(dataset,test_size=0.25, random_state=130)
  return train,test

def train_dataloader(train_dataset,sampler):
  training_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, sampler=sampler)
  return training_data_loader

def test_dataloader(test_dataset,sampler):
  test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, sampler=sampler)
  return test_data_loader

def validation_dataloader(validation_dataset,sampler):
  validation_data_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=100, sampler=sampler)
  return validation_data_loader

dataset = import_data()

train_dataset , test_dataset = my_train_test_split(dataset)

#train_loader = train_dataloader(train_dataset)
#test_loader = test_dataloader(test_dataset)

print(len(train_dataset))
print(len(test_dataset))

class Mask_Detection_CNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv_layer_1 = nn.Sequential(
            nn.Conv2d(3,16,3,1,padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2,2)
            
            
        )
        self.conv_layer_2 = nn.Sequential(
            nn.Conv2d(16,32,3,1,padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2,2)
            
        )
        self.conv_layer_3 = nn.Sequential(
            nn.Conv2d(32,64,3,1,padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2,2)
            
            
        )
        self.conv_layer_4 = nn.Sequential(
            nn.Conv2d(64,128,3,1,padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2,2)
            
        )

        

        self.Neural_layer_1 = nn.Linear(128*4*4,2048)
        self.Neural_layer_2 = nn.Linear(2048,1024)
        self.Neural_layer_3 = nn.Linear(1024,512)
        self.Neural_layer_4 = nn.Linear(512,5)

       

    def forward(self,x):
        x = self.conv_layer_1(x)
        x = self.conv_layer_2(x)
        x = self.conv_layer_3(x)
        x = self.conv_layer_4(x)
        x = x.view(-1,128*4*4)
        x = F.relu(self.Neural_layer_1(x))
        x = F.relu(self.Neural_layer_2(x))
        x = F.relu(self.Neural_layer_3(x))
        x = self.Neural_layer_4(x)
        
        
        return x

model = Mask_Detection_CNN()

Epochs = 10
Learning_Rate = 0.01
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(),lr = Learning_Rate)
train_loss=0
correct_results=0

def metrics_and_scores(y_pred,y_true):
  cfm=confusion_matrix(y_true,y_pred)
  
  df_cfm = pd.DataFrame(cfm, index = [i for i in classes],
                     columns = [i for i in classes])
  plt.figure(figsize = (12,7))
  fig=sn.heatmap(df_cfm, annot=True)
  figure = fig.get_figure()
  figure.savefig("heatmap.png")
  
  true_positive = np.diag(cfm)
  precision = list(map(truediv, true_positive, np.sum(cfm, axis=0)))
  
  for i in range(len(precision)):
    if math.isnan(precision[i]):
      precision[i] = float("{:.2f}".format(precision[i]))
    if math.isnan(precision[i]):
      precision[i]=0.00

  recall = list(map(truediv, true_positive, np.sum(cfm, axis=1)))

  for i in range(len(recall)):
    if math.isnan(recall[i]):
      recall[i] = float("{:.2f}".format(recall[i]))
    if math.isnan(recall[i]):
      recall[i]=0.00

  F1=[]

  for i in range(len(classes)):
    F_score = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])
    F1.append(F_score)

  
  return cfm

def loss_plt(Epochs,loss_epochs):
  plt.xlabel("Number of epochs")
  plt.ylabel("Average Loss Value")
  plt.plot(Epochs,loss_epochs)
  plt.title('Loss And Number of epochs')
  plt.show()
  plt.close()

def accuracy_plt(Epochs,acc_epochs):
  plt.xlabel("Number of epochs")
  plt.ylabel("Average Accuracy Value")
  plt.plot(Epochs,acc_epochs)
  plt.title('Accuracy And Number of epochs')
  plt.show()
  plt.close()

def train_CNN_Model(train_loader_data):
  train_loss=0
  correct_results=0
  total_size=0
  y_pred=[]
  y_true=[]
  model.train()

  for epoch in range(Epochs):
      for i, (images, labels) in enumerate(train_loader_data):

          #images, labels = images.to(device), labels.to(device)
        
          result = model(images)
          loss = criterion(result, labels)

          optimizer.zero_grad()
          loss.backward()
          optimizer.step()

          _, preds = torch.max(result, 1)
          train_loss += loss.item()
          correct_results += torch.sum(preds == labels.data)

          total_size += labels.size(0)

          acc = 100.0 * correct_results / total_size

          

  return train_loss,correct_results

from torch.functional import Tensor
def train_CNN_Model_b():
  train_loss=0
  correct_results=0
  total_size=0
  y_pred=[]
  y_true=[]
  loss_vals = []
  acc_vals =[]
  
  for epoch in range(Epochs):
      loss_epochs= []
      for i, (images, labels) in enumerate(finalDataLoader_1):
          images,labels = images.to(device),labels.to(device)

          result = model_b(images)
          loss = criterion(result, labels)
          optimizer_b.zero_grad()
          loss.backward()
          loss_epochs.append(loss.item())
          optimizer_b.step()

          _, preds = torch.max(result, 1)
          train_loss += loss.item()
          correct_results += torch.sum(preds == labels.data)

          total_size += labels.size(0)

          acc = 100.0 * correct_results / total_size
          
          if (i+1) % len(finalDataLoader_1) == 0:
              print (f'Epoch [{epoch+1}/{Epochs}], Loss: {loss.item():.4f}')
              print('accuracy is: {:.4f} '.format(acc))
              acc_vals.append(acc)
      loss_vals.append(sum(loss_epochs)/len(loss_epochs))
  print('Finished Training')
  PATH = './Cnn_Model'+'.pth'
  torch.save(model_b.state_dict(), PATH)

  loss_plt(np.linspace(1, Epochs, Epochs).astype(int),loss_vals)
  accuracy_plt(np.linspace(1, Epochs, Epochs).astype(int) ,acc_vals)

def test_CNN_Model(test_loader_data):
  with torch.no_grad():
      correct_result = 0
      total_size = 0
      test_loss = 0
      for images, labels in test_loader_data:
          #images, labels = images.to(device), labels.to(device)
          result = model(images)
          loss = criterion(result, labels)

          _, preds = torch.max(result, 1)
          test_loss += loss.item()
          total_size += labels.size(0)
          correct_result += (preds == labels).sum().item()

          output = (torch.max(torch.exp(result), 1)[1]).data.cpu().numpy()
          y_pred.extend(output)

          labels = labels.data.cpu().numpy()
          y_true.extend(labels)
      print(classification_report(y_true, y_pred, target_names=classes))  


  return test_loss,correct_result


def test_CNN_Model_b(t_loader,category):
    y_pred=[]
    y_true=[]
    with torch.no_grad():
      correct_result = 0
      total_size = 0
      for images, labels in t_loader:
          images,labels = images.to(device),labels.to(device)
          result = model_b(images)

          _, preds = torch.max(result, 1)
          total_size += labels.size(0)
          correct_result += (preds == labels).sum().item()

          output = (torch.max(torch.exp(result), 1)[1]).data.cpu().numpy()
          y_pred.extend(output)

          labels = labels.data.cpu().numpy()
          y_true.extend(labels)
  

      acc = (100.0 * correct_result) / total_size
      acc = "{:.2f}".format(acc)
      print(f'Accuracy of the network: {acc} %')

      metrics_and_scores(y_pred,y_true,category)
      print(classification_report(y_true, y_pred, target_names=classes))

def validation_CNN_Model(validation_loader_data):
  with torch.no_grad():
  
    correct_result = 0
    total_size = 0
    validation_loss = 0
    for images, labels in validation_loader_data:
        #images, labels = images.to(device), labels.to(device)
        result = model(images)
        loss = criterion(result, labels)

        _, preds = torch.max(result, 1)
        validation_loss += loss.item()
        total_size += labels.size(0)
        correct_result += (preds == labels).sum().item()


    return validation_loss,correct_result

metrics_and_scores(y_pred,y_true)
print(classification_report(y_true, y_pred, target_names=classes))
loss_plt(np.linspace(1, Epochs, Epochs).astype(int),loss_vals)
accuracy_plt(np.linspace(1, Epochs, Epochs).astype(int),acc_vals)

from PIL import Image
model=torch.load('/content/drive/MyDrive/k_cross_CNN (1).pt',map_location=torch.device('cpu'))

img_path = "/content/drive/MyDrive/Final_DATASET_LAST/N95WithValve/0040.jpg"
image = Image.open(img_path)
image_tensor = transform(image)
image = image_tensor.view(1,3,64,64)
output = model(image)
index = output.data.numpy().argmax()
pred = classes[index]
print(pred)

import os
def main():
    test_loader_lst = []
    train_loader_lst = []
    finalDataLoader = None
    Folder_name_lst = []
    train_dataset_lst = []
    dataset_path= "/content/drive/MyDrive/"
    for folder in os.listdir(dataset_path):
      Folder_name_lst.append(folder)
      dataset = import_data(dataset_path+folder)
      train_dataset,test_dataset = my_train_test_split(dataset)
      train_dataset_lst.append(train_dataset)
      train_loader = train_dataloader(train_dataset)
      train_loader_lst.append(train_loader)
      test_loader=  test_dataloader(test_dataset)
      test_loader_lst.append(test_loader)
      print("Len of Train Loader of "+folder+" is "+str(len(train_loader)))
      print("Len of Test Loader of "+folder+" is "+str(len(test_loader)))
    finalDataSet = torch.utils.data.ConcatDataset([train_dataset_lst[0],train_dataset_lst[1],train_dataset_lst[2],train_dataset_lst[3]])
    global finalDataLoader_1
    finalDataLoader_1 = train_dataloader(finalDataSet)
    global model_b
    model_b = Mask_Detection_CNN()
    global optimizer_b
    optimizer_b = torch.optim.Adam(model_b.parameters(),lr = Learning_Rate)
    train_CNN_Model_b()
    for i in range(0,4):
      test_CNN_Model_b(test_loader_lst[i],Folder_name_lst[i])

main()